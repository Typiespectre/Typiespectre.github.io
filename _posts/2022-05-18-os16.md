---
title: "OSTEP - 가상화: 메모리 크기의 극복: 정책"
lang: "ko"
layout: post
date: 2022-05-18 16:10:58 +0900
categories: [os]
tags: [os]
---

페이지 단위로 데이터를 물리 메모리에 넣는 일은, 만약 메모리에 빈 공간이 거의 없을 경우 복잡한 처리 과정이 요구된다. **메모리 압박(memory pressure)**을 해소하기 위해 운영체제는 특정한 **교체 정책(replacement policy)**을 바탕으로 **내보낼(evict)** 페이지를 선택한 후, 선택된 페이지를 페이징 아웃(paging out)하여 공간을 확보해야 한다.

이러한 물리 메모리와 페이지의 관계는, 메인 메모리를 시스템의 가상 메모리 페이지의 **캐시**처럼 생각하게 할 수 있는데, 따라서 교체 정책의 목표는 그러한 **캐시 미스**의 횟수를 최소화하는 것이라고 말할 수 있을 것이다. 캐시 히트와 미스의 횟수를 안다면, 프로그램의 **평균 메모리 접근 시간(average memory access time, AMAT)**을 계산할 수 있다:

```
AMAT = T_M + (P_Miss * T_D)

T_M : 메모리 접근 비용
T_D : 디스크 접근 비용
P_Miss : 캐시 미스
```
* 메모리의 데이터를 접근하는 비용은 항상 지불해야 하며, 메모리에서 데이터를 못 찾을 경우에는, 디스크로부터 데이터를 가져오는 비용을 추가로 지불해야 한다.
* 현대 시스템에서는 디스크 접근 비용이 아주 크기에, 캐시 미스를 최대한 줄여야 한다.

그렇다면, 캐시 미스를 최대한 줄일 수 있는 교체 정책은 무엇이 있을까?

## 최적 교체 정책(The Optimal Replacement Policy)

Belady가 개발한 최적 교체 정책은 미스를 최소화하는 정책인데, 그 이유는 이후에 접근할 페이지의 목록을 훑어본 후, *가장 나중에 접근될* 페이지를 교체하기 때문이다. 하지만 이러한 스케줄링 정책을 실제로 구현하기 위해선 미래를 내다보아야 한다. 하지만 이는 컴퓨터에서는 불가능하므로, 최적 기법은 다른 교체정책의 비교 기준으로만 사용될 것이다(즉, 이 정책의 결과에 가까울수록 "정답"에 가깝다!).

```
정책: OPT, 캐시 사이즈: 3
refered  HIT/MISS?  evict  queue
   0       MISS                0 -+
   1       MISS              0,1  |-> cold-start miss
   2       MISS            0,1,2 -+
   0       HIT             0,1,2
   1       HIT             0,1,2
   3       MISS       2    0,1,3
   0       HIT             0,1,3
   3       HIT             0,1,3
   1       HIT             0,1,3
   2       MISS       3    0,1,2
   1       HIT             0,1,2

총 접근: 11, 캐시 미스: 5, 캐시 히트: 6
```

## 선입선출(FIFO)

간단하고 구현하기 쉬운 정책으로, 선입선출(FIFO) 정책이 있다. 이 방법은 시스템 큐에 들어온 순서대로 페이지를 교체한다(즉 *"먼저 들어온" 페이지*가 다음에 교체되어야 할 페이지가 된다). 하지만 FIFO는 페이지의 중요도를 파악하지 못하기에, 어떤 페이지가 중요하게 사용되더라도(어떤 페이지가 이후에도 자주 사용되더라도), 그러한 페이지를 교체해버림으로써 낮은 효율성을 보이게 될 수 있다.

```
정책: FIFO, 캐시 사이즈: 3
refered  HIT/MISS?  evict  queue
   0       MISS                0
   1       MISS              0,1
   2       MISS            0,1,2
   0       HIT             0,1,2
   1       HIT             0,1,2
   3       MISS       0    1,2,3
   0       MISS       1    2,3,0
   3       HIT             2,3,0
   1       MISS       2    3,0,1
   2       MISS       3    0,1,2
   1       HIT             0,1,2

총 접근: 11, 캐시 미스: 7, 캐시 히트: 4
```

* Belady's Anomaly : 일반적으로 캐시의 크기가 커지면 캐시 히트율이 증가하지만, FIFO는 그렇지 않다. 왜냐하면 FIFO는 스택의 특성을 따르지 않기에, 이전 캐시의 내용을 포함하지 않기 때문이다. 좀 더 쉽게 말하면, 히스토리를 매번 지워버리고 새로 시작하게 된다고 말할 수 있을까? 만약 캐시 크기가 3이고, 이후에 주어지는 참조 흐름이 '1, 2, 3, 4, 1, 2, 5, 1, 2, 3, 4'라고 해보자. 캐시 크기가 4가 되어도 여전히 캐시 미스는 동일하게 된다(모두 미스!).

## 무작위 선택

FIFO처럼 구현하기 쉬운 교체 방식이지만, FIFO와 동일하게, 블럭(페이지)의 중요도를 파악하지 못한다. 하지만 운이 좋다면 좋은 효율성을 보일 수도 있지만(무작위로 선택된 페이지가 아주 운이 좋게 최적의 방식을 따르는 경우), 말 그대로 "운"에 따라 아주 나쁜 결과를 가질 수도 있다.

## LRU(Least-Recently-Used)

최적 기법의 '미래에 대한 예측'을 가능한 가깝게 구현하기 위해 *과거 사용 이력*을 활용한다.

페이지 교체 정책이 활용할 수 있는 과거 정보는 **빈도수(frequency)**와 **최근성(recency)**이다. 한 페이지가 여러 차례 접근되었다면, 프로그램에서 중요한 역할을 하고 있을 확룔이 높기에 교체되면 안될 것이라고 추측할 수 있다. 또한 최근에 접근된 페이지일수록 가까운 미래에 접근될 확률이 높다. 이들은 프로그램의 **지역성의 원칙(principle of locality)** 특성에 기반을 둔다.

과거 이력에 기반한 교체 알고리즘으로, *가장 적은 빈도*로 사용된 페이지를 교체하는 **Least-Frequently-Used(LFU)** 정책과, *가장 오래 전에* 사용된 페이지를 교체하는 **Least-Recently-Used(LRU)** 정책이 있다.

LRU는 최근에 접근되었던(캐시 히트되었던) 페이지를 큐에 유지하고, 그 유지 시간이 가장 긴 페이지를 다음 교체 대상으로 삼는다. LRU는 최적 기법과 같은 정도의 수준의 성능을 보여줄 수 있다.

```
정책: LRU, 캐시 사이즈: 3
refered  HIT/MISS?  evict  queue
   0       MISS                0
   1       MISS              0,1
   2       MISS            0,1,2
   0       HIT             1,2,0
   1       HIT             2,0,1
   3       MISS       2    0,1,3
   0       HIT             1,3,0
   3       HIT             1,0,3
   1       HIT             0,3,1
   2       MISS       0    3,1,2
   1       HIT             3,2,1

총 접근: 11, 캐시 미스: 5, 캐시 히트: 6
```

## 워크로드에 따른 성능 비교

하지만 모든 경우에서 LRU가 최선을 보장하는 것은 아니다. 즉, 지역성이 없는 워크로드라면, 어느 정책을 사용하든 상관이 없다. 만약 인기 있는 페이지와 그렇지 않은 페이지가 나눠진 경우(80 대 20 워크로드), 인기 있는 페이지들을 캐시에 더 오래 두는 경향이 있는 LRU가 좋은 성능을 보인다(그러나 최적 기법에 완전히 미치지는 않는다).

그러나 "순차 반복" 워크로드의 경우(1, 2, 3, 4, 5, 6, 7, 8 ...), LRU는 FIFO와 동일하게 캐시 히트율 0%를 기록한다. 이 경우, 오히려 무작위 선택이 더 나은 성능을 보인다. 무작위 선택의 장점은, 이러한 이상한 엣지 케이스를 방지할 수 있다는 점이다.

## 과거 이력 기반 알고리즘의 구현

LRU는 전체적으로 좋은 성능을 보여줄 수 있지만, 이를 구현하는 일은 또 다른 문제이다. LRU는 어떤 페이지가 가장 최근에 또는 오래전에 접근되었는지를 관리하기 위해, *모든 메모리 참조 정보*를 기록해야 하는데, 이를 구현하기 위해 약간의 하드웨어 지원을 받아, 페이지 접근이 일어날 때마다, 하드웨어가 메모리의 시간 필드를 갱신하게 할 수 있다.

그러나 시스템의 페이지 수가 증가하면 페이지들의 시간 정보 배열을 검색하여, 가장 오래 전에 사용된 페이지를 찾는 것은 매우 고비용의 연산이 된다. 따라서 현대 시스템에서는 LRU의 정책을 근사하는 방식으로 구현이 이루어지게 된다.

## LRU 정책 근사하기

LRU 정책을 근사하기 위해, 시스템의 각 페이지마다 **use bit(또는 reference bit)**을 설정하고, 페이지가 참조될 때마다 하드웨어는 use bit을 1로 변경한다.

운영체제는 페이지의 use bit을 **시계 알고리즘(clock algorithm)**을 바탕으로 검사하는데, 마치 시계 바늘이 원형의 시계 숫자를 계속 도는 것처럼, 만약 현재 페이지의 use bit가 1이라면 현재 페이지는 최근에 사용되었으므로, 바람직한 교체 대상이 아님으로 판단하고 넘어가는 대신, 해당 페이지의 use bit를 0으로 초기화하고, 운영체제는 use bit가 0인 페이지, 즉 최근에 사용된 적이 없는 페이지를 찾을 때까지 위의 과정을 반복한다. (변형된 시계 알고리즘, 즉 각 페이지를 랜덤으로 검사하는 방식도 가능하다.)

## 갱신된 페이지(Dirty Page)의 고려

이러한 고려는 시계 알고리즘을 개선할 수 있다. 만약 어떤 페이지가 **변경(modified)**되어 **더티(dirty)** 사앹가 되었다면, 그 페이지를 내보내기 위해선 디스크에 변경 내용을 기록해야 하기에 비싼 비용을 지불해야 한다. 반대로 변경되지 않았다면(즉, clean하다면), 해당 페이지 프레임은 추가적인 비용 없이 내보낼 수 있다.

따라서 하드웨어는 **modified bit(또는 dirty bit)**를 추가로 포함하여, 페이지가 변경될 때마다 이 비트를 1로 설정하고, 운영체제는 페이지 교체 알고리즘에서 해당 비트를 고려하여 고체 대상을 선택할 수 있다. 예를 들어, 시계 알고리즘은 교체 대상을 선택할 때, 사용되지 않은 상태이고 깨끗한, 두 조건을 모두 만족하는 페이지를 먼저 찾도록 수정될 수 있다.

## 다른 VM 정책들

페이지 교체 정책만이 VM 시스템이 채탁하는 유일한 정책은 아니다(비록 중요하긴 하지만). 운영체제는 언제 페이지를 불러들일지 결정해야 하는데, 이를 **페이지 선택(page selection)** 정책이라고 한다.

페이지 선택 정책으로, 시스템이 특정 페이지를 요청하면 즉시 반입하는 **요구 페이징(demand paging)** 정책이 있으며, 운영체제는 대부분 이러한 정책을 사용한다. 운영체제는 이러한 동작에 **선반입(prefetching)**을 적용하여, 요청된 페이지 외에도 곧 읽어들여야 할 것으로 예측되는 다른 페이지도 한번에 메모리로 읽어들일 수 있다. 또는 기록해야 할 페이지를 메모리에 모은 후, 한 번에 디스크에 기록하는 **클러스터링(clustring) 또는 쓰기 모으기(grouping of writes)** 동작 방식을 수행할 수도 있다.

## 쓰래싱(Thrashing)

메모리 사용 요구가 감당할 수 없을 만큼 많고, 실행 중인 프로세스가 요구하는 메모리가 가용 물리 메모리 크기를 초과하는 경우, 시스템은 끊임없이 페이징을 할 수밖에 없고, 이와 같은 상황을 쓰레싱이라 부른다.

## 요약

현대 시스템들은 시계 알고리즘과 같은 LRU에 근사한 방법에 몇 가지 성능 향상을 위한 기능을 추가하였다. 그 중 하나가 **탐색 내성(scan resistance)**인데, 이는 LRU가 순차 반복 워크로드와 같은 최악의 경우에 보이는 행동을 방지한다.

그러나 지금까지 언급했던 알고리즘의 중요성은 점차 퇴색되고 있다고 할 수 있는데, 메모리 접근 시간과 디스크 접근 시간의 차이가 점차 증가하고 있기 때문이다. 과도한 페이징에 대한 최적의 해결책은, 더 많은 메모리를 구입하는 것으로 아주 간단하게(지적으로는 불만족스럽게) 해결될 수 있기 때문이다.
