---
title: "OSTEP - 병행성: 락 기반의 병행 자료 구조"
lang: "ko"
layout: post
date: 2022-07-30 21:04:36 +0900
categories: [os]
tags: [os]
---

경쟁 조건으로부터 안전한 **쓰레드 안전(thread sage)** 자료 구조를 만드는 방법은 어떻게 될까? 우선, 가장 보편적으로 사용되면서 인터페이스가 간단한 자료 구조인 카운터를 살펴보자.

## 병행 카운터

카운터를 쓰레드 안전하게 만드는 코드는 아래와 같다:

```c
typedef struct __ounter_t {
	int		value;
	pthread_mutex_t	lock;
} counter_t;

void	init(counter_t *c) {
	c->value = 0;
	Pthread_mutex_init(&c->lock, NULL);
}

void	increment(counter_t *c) {
	Pthread_mutex_lock(&c->lock);
	c->value++;
	Pthread_mutex_unlock(&c->lock);
}

void	decrement(counter_t *c) {
	Pthread_mutex_lock(&c->lock);
	c->value--;
	Pthread_mutex_unlock(&c->lock);
}

int	get(counter_t *c) {
	Pthread_mutex_lock(&c->lock);
	int	rc = c->value;
	Pthread_mutex_unlock(&c->lock);
	return rc;
}
```
<br />

위 카운터는 병행 자료 구조의 기본적인 디자인 패턴을 가진다. 자료 구조를 조작하는 루틴을 호추할 때 락을 추가하고, 호출문이 리턴될 때 락이 해제된다. (이러한 기법은 객체에 대한 메소드를 호출하고 리턴할 때 자동적으로 락을 획득하고 해제하는 **모니터(monitor)** 기법과 유사하다.)

위 병행 자료 구조는 제대로 동작하지만, 성능이 좋지 않다. 쓰레드 개수가 늘어날수록 성능은 나빠진다. 작업양이 CPU의 개수에 비례하여 증가하여도 각 작업이 병렬적으로 처리되어 전체 완료 시간이 늘어나지 않는 **완벽한 확장성(pefrect scaling)**이 병행 자료 구조에 요구된다.

확장성 있는 카운팅 방법 중, **근사 카운터(approximate counter)**라고 불리는 기법이 있다.근사 카운터는 CPU 코어마다 존재하는 하나의 물리적인 *지역* 카운터와 하나의 *전역* 카운터로 구성되어 있다. 그리고 지역 카운터를 위한 락들과 전역 카운터를 위한 락이 존재한다.

근사 카운터는 다음과 같이 동작한다:
- 쓰레드는 지역 카운터를 증가시킨다.
	- 지역 카운터는 지역 락에 의해 보호된다.
	- 각 CPU는 저마다 지역 카운터를 갖기 때문에 CPU들에 분산되어 있는 쓰레드들은 지역 카운터를 경쟁 없이 갱신할 수 있다.
- 쓰레드는 전역 카운터를 읽어서 카운터 값을 판단한다.
	- 전역 카운터의 값은 주기적으로 지역 카운터 값을 판단하여 갱신되어야 한다.
	- 전역 락을 사용하여 지역 카운터의 값을 전역 카운터의 값에 더하고, 그 지역 카운터의 값은 0으로 초기화한다.
- 지역에서 전역으로 값을 전달하는 빈도는 정해놓은 S 값에 의해 결정된다.
	- S의 값이 작을수록(갱신 빈도가 클수록) 카운터의 확장성이 없어지며, S의 값이 클수록(갱신 빈도가 작을수록) 전역 카운터의 값과 실제 카운터의 값이 일치하지 않을 확률이 커진다.

근사 카운터의 *대략적인* 코드는 아래와 같다:

```c
typedef struct __counter_t {
	int		global;	// 전역 카운터
	pthread_mutex_t	glock;	// 전역 카운터
	int		local[NUMCPUS];	// cpu당 지역 카운터
	pthread_mutex_t	llock[NUMCPUS];	// ... 그리고 락들
	int		threshold;	// 갱신 빈도
} counter_t;

// init: 한계치를 기록하고 락과 지역 카운터 그리고 전역 카운터의 값들을 초기화함
void	init(counter_t *c, int thresohld) {
	c->threshold = threshold;
	c->global = 0;
	pthread_mutex_init(&c->glock, NULL);
	int	i;
	for (i = 0; i < NUMCPUS; i++) {
		c->local[i] = 0;
		pthread_mutex_init(&c->lock[i], NULL);
	}
}

// update: 보통은 지역 락을 획득한 후 지역 값을 갱신함
// '한계치' 까지 지역 카운터 값 증가시, 전역 락 획득 후 지역 값을 전역 카운터에 전달함
void	update(counter_t *c, int threadID, int amt) {
	int	cpu = threadID % NUMCPUS;
	pthread_mutex_lock(&c->llock[cpu]);
	c->local[cpu] += amt;
	if (c->local[cpu] >= c->threshold) {
		// 전역으로 전달(amt > 0 가정)
		pthread_mutex_lock(&c->glock);
		c->global += c->local[cpu];
		pthread_mutex_unlock(&c->glock);
		c->local[cpu] - 0;
	}
	pthread_mutex_unlock(&c->llock[cpu]);
}

// get: 전역 카운터의 값을 리턴(근사 값)
int	get(counter_t *c) {
	pthread_mutex_lock(&c->glock);
	int	val = c->global;
	pthread_mutex_unlock(&c->glock);
	return val;	// 근사 값임!
}
```
<br />

## 병행 연결 리스트

병행 연결 리스트(기본적인 삽입 연산만 구현함)는 아래와 같이 구현할 수 있다. 다만 `malloc()`이 실패하는 경우, 실패를 처리하기 전에 락을 해제해야 한다는 추가 조건을 염두해두자(즉 에러로 인한 실행 중지에 각별하게 주의를 해야 한다). 버그(락을 해제하지 않고 리턴하는 경우)가 발생하지 않도록 `malloc()`을 쓰레드 안전하게 만들고, 공유 리스트 갱신 때만 락을 획득하게 하자.

```c
// 기본 노드 구조
typedef struct __node_t {
	int		key;
	struct __node_t	*next;
} node_t;

// 기본 리스트 구조 (리스트마다 하나씩 사용)
typedef struct	__list_t {
	node_t		*head;
	pthread_mutex_t	lock;
} list_t;

void	List_Init(list_t *L) {
	L->head = NULL;
	pthread_mutex_init(&L->lock, NULL);
}

void	List_Insert(list_t *L, int key) {
	node_t	*new = malloc(sizeof(node_t));
	if (new == NULL) {
		perror("malloc");
		return ;	// 실패
	}
	new->key = key;
	// 임계 영역만 락으로 보호
	pthread_mutex_lock(&L->lock);
	new->next = L->head;
	L->head = new;
	pthread_mutex_unlock(&L->lock);
}

int	List_Lookup(list_t *L, int key) {
	int	rv = -1;
	pthread_mutex_lock(&L->lock);
	node_t	*curr = L->head;
	while (curr) {
		if (curr->key == key) {
			rv = 0;
			break;
		}
		curr = curr->next;
	}
	pthread_mutex_unlock(&L->lock);
	return rv;	// 성공과 실패를 동시에 나타냄.
}
```
<br />

하지만 역시 확장성이 좋지 않다는 문제가 있다. 병행성을 개선하는 방법 중 하나로 **hand-over-hand locking(lock coupling)**이라는 기법이 개발되었다. 이 기법은 전체 리스트에 하나의 락이 있는 것이 아니라 개별 노드마다 락을 추가하는 것이다. 리스트를 순회할 때 다음 노드의 락을 먼저 획득하고 지금 노드의 락을 해제하도록 한다.

하지만 개념적으로는 리스트 연산에 병행성이 높아지기 때문에 성능이 나아질 것이라고 예상되지만, 실제로는 위의 코드처럼 하나의 락을 두는 방법에 비해 속도 개선이 쉽지 않다고 한다. 왜냐하면 리스트를 순회할 때 각 노드에서 락을 획득하고 해제하는 오버헤드가 매우 크기 때문이다. 일정 개수의 노드를 처리할 때마다 하나의 새로운 락을 획득하는 하이브리드 방식이 더 나을 수도 있다.

## 병행 큐

어떤 자료 구조를 병행 연산에 대해 보호하기 위한 가장 쉬운 방법은 자료 구조 전체에 커다란 락을 하나 두는 것이다. 이 방법은 쉽지만, 다음과 같이 병행성이 더 좋은 큐를 구현할 수도 있다. 병행 큐는 큐의 헤드와 큐의 테일에 락이 하나씩 있다. 삽입 연산에서 테일 락이 접근되고, 추출 연산에서 헤드 락이 접근된다. 병행 큐는 멀티 쓰레드 프로그램에서 자주 사용되지만, 여기에서 소개된 큐는 실전에선 사용할 수 없다. 큐가 비었거나 가득 찬 경우, 쓰레드가 대기하도록 하는 기능이 필요하기 때문이다.

```c
typedef struct	__node_t {
	int		value;
	struct __node_t	*next;
} node_t;

typedef struct	__queue_t {
	node_t		*head;
	node_t		*tail;
	pthread_mutex_t	headLock, tailLock;
} queue_t;

void	Queue_Init(queue_t, *q) {
	node_t	*tmp = malloc(sizeof(node_t));
	tmp->next = NULL;
	q->head = q->tail = tmp;
	pthread_mutex_init(&q->headLock, NULL);
	pthread_mutex_init(&q->tailLock, NULL);
}

void	Queue_Enqueue(queue_t *q, int value) {
	node_t	*tmp = malloc(sizeof(node_t));
	assert(tmp != NULL);
	tmp->value = value;
	tmp->next = NULL;

	pthread_mutex_lock(&q->tailLock);
	q->tail->next = tmp;
	q->tail = tmp;
	pthread_mutex_unlock(&q->tailLock);
}

int	Queue_Deququq(queue_t *q, int *value) {
	pthread_mutex_lock(&q->headLock);
	node_t	*tmp = q->head;
	node_t	*newHead = tmp->next;
	if (newHead == NULL) {
		pthread_mutex_unlock(&q->headLock);
		return -1;	// 큐가 비어 있음
	}
	*value = newHead->value;
	q->head = newHead;
	pthread_mutex_unlock(&q->headLock);
	free(tmp);
	return 0;
}
```
<br />

## 병행 해시 테이블

아래의 병행 해시 테이블은 전체 자료 구조에 하나의 락을 사용한 것이 아니라 해시 버켓(리스트로 구현되어 있음)마다 락을 사용하여 병행성이 좋다.

```c
#define BUCKETS (101)

typedef struct	__hash_t {
	list_t	lists[BUCKETS];
} hash_t;

void	Hash_Init(hash_t *H) {
	int	i;
	for (i = ; i < BUCKETS; i++) {
		List_Init(&H->lists[i]);
	}
}

int	Hash_Insert(hash_t *H, int key) {
	return List_Insert(&H->lists[key % BUCKETS], key);
}

int	Hash_Lookup(hash_t *H, int key) {
	return List_Lookup(&H->lists[key % BUCKETS]. key);
}
```
<br />

## 요약

병행 자료 구조에서 우리가 얻을 수 있는 교훈은...
- 락 획득과 해제 시 코드를 세심히 살펴보자(버그가 발생하지 않도록).
- 병행성 개선이 반드시 성능 개선은 아니다.
- 성능 개선은 성능에 문제가 생길 경우에만 해결책을 간구하자.
- **미숙한 최적화(premature optimization)**를 피하자.
	- 병행 자료 구조를 만들 떄에는 하나의 큰 락을 추가하여 동기화 접근을 제어하는 가장 간단한 방법에서 시작하자.
	- "미숙한 최적화는 모든 악의 근원이다."
	- 부분적인 성능 개선 시도가 응용 프로그램의 전체 성능을 개선하지 못한다면 아무 의미가 없다.
